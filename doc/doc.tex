
\documentclass[a4paper,UKenglish,cleveref, autoref, thm-restate]{lipics-v2019}
%This is a template for producing LIPIcs articles. 
%See lipics-manual.pdf for further information.
%for A4 paper format use option "a4paper", for US-letter use option "letterpaper"
%for british hyphenation rules use option "UKenglish", for american hyphenation rules use option "USenglish"
%for section-numbered lemmas etc., use "numberwithinsect"
%for enabling cleveref support, use "cleveref"
%for enabling autoref support, use "autoref"
%for anonymousing the authors (e.g. for double-blind review), add "anonymous"
%for enabling thm-restate support, use "thm-restate"

%\graphicspath{{./graphics/}}%helpful if your graphic files are in another directory

\bibliographystyle{plainurl}% the mandatory bibstyle

\title{Documentation of floatsmt}

\author{Felix Griesau}{Karlsruhe Institute of Technology, Germany}{TODO}{}{}
\author{Marc Huisinga}{Karlsruhe Institute of Technology, Germany}{mhuisi@protonmail.com}{}{}

\authorrunning{F. Griesau and M. Huisinga}

\Copyright{Felix Griesau and Marc Huisinga}

\ccsdesc[500]{Theory of computation~Automated reasoning}

\keywords{Automated theorem proving, SMT solving, Z3, Floating point, IEEE 754, smtlib}

\category{}

\relatedversion{}

\supplement{\url{https://github.com/mhuisi/float-smt}}

\acknowledgements{}

\nolinenumbers

\hideLIPIcs 

\lstset{language=Python}

\begin{document}

\maketitle

\begin{abstract}
This report documents the floatsmt library, which implements a floating point theory using the Z3 SMT solver. 
\end{abstract}

% Use \autoref for references and lstlisting for code.

\section{Introduction}
The IEEE 754 floating point standard \cite{ieee} is complex. Manually ensuring the correctness of programs that use floating point operations requires great care: 
\begin{itemize}
\item Edge cases like $+0$, $-0$, $+\infty$, $-\infty$ and \verb|NaN| need to be handled correctly, since all operations behave differently when applied to one of these edge cases.
\item Rounding errors need to be kept in check. Depending on the operation, the values and even the rounding mode involved, rounding errors may grow very large.
\item Operations on normals and subnormals can unintentionally produce one of the above edge cases in the situation of an underflow or an overflow.
\end{itemize}
Aside from these issues, floating point operations can always be understood in terms of their semantics, where a float is mapped to its real number value, the corresponding operation is performed over the reals and the result is rounded and mapped back to the floating point numbers.

Unfortunately, implementations of the floating point standard cannot easily and efficiently perform operations on the real numbers. Hence, implementations need to perform operations on floats using bit vectors and ensure that the result is still rounded correctly. This is even more error-prone than the semantical view of floats that users of these implementations work with, and very costly if erroneous\footnote{See \url{https://web.archive.org/web/20190618044444/http://www.trnicely.net/pentbug/pentbug.html} for an infamous example}. 

To combat these complexities, verification tools like SMT solvers have recently begun supporting floating point theories \cite{semantics}. These theories can be used to prove properties about implementations of the floating point standard and programs that use floats, ensuring correctness in regard to these properties.

But who watches the watchmen? How do we guarantee that the SMT solver implementation of the floating point theory is itself correct? After all, such an SMT solver implementation needs to surpass the same complexities as other floating point implementations.

To help reduce this uncertainty in the sense of ``trust, but verify'', we implemented floatsmt, a floating point theory for the Z3 SMT solver \cite{z3}. floatsmt was implemented from scratch, not based on any reference implementation and then verified against Z3's own floating point theory. In our verification, we found no bugs in Z3's theory, gaining a significant amount of trust in the floating point theory of Z3 in the process. This report details our decisions for the design and implementation of floatsmt.

\section{Usage instructions}
floatsmt is a Python library for the Z3 Python interface ``z3py''.

\subsection{Installation}
\begin{enumerate}
\item Install Python ($\geq$ 3.7.4) (\url{https://www.python.org/downloads/}).
\item Install the z3-solver package ($\geq$ 4.8.8.0) via \verb|pip install z3-solver| (\url{https://pypi.org/project/z3-solver/}).
\item Install git (\url{https://git-scm.com/}).
\item Clone the repository via \verb|git clone https://github.com/mhuisi/float-smt|.
\item Copy the \verb|floatsmt| directory to where the library will be used.
\item Add \verb|import floatsmt| to the top of the Python file where the library will be used and get started.
\end{enumerate} 

\subsection{Usage}
What follows is a code snippet to show how the library is used.
\begin{lstlisting}
from floatsmt.api import *

# Sets the rounding mode globally.
# Must be one of 
# {NearestTiesToEven,NearestTiesAwayFromZero,Up,Down,Truncate}.
set_default_rm(NearestTiesToEven)

# Create the floating point sort which we will use.
# floatsmt supports arbitrary bit widths.
# The first argument denotes the mantissa width,
# the second argument denotes the exponent width.
# (23, 8) is a standard 32 bit float.
sort = FloatSort(23, 8)

# Create a float constant from a specific bit representation.
x = SMTFloat.FloatVal(0, 0b1011, 0b1111, sort)
# Create a float constant from a decimal representation. 
# The mode for rounding from decimal to binary must be one of
# {NEAREST_TIE_TO_EVEN,NEAREST_TIE_AWAY_FROM_ZERO,UP,DOWN,TRUNCATE}.
y = SMTFloat.FloatValDec("0.123e-2", 
      converter.RoundingMode.NEAREST_TIE_TO_EVEN, sort)

# Many operations can be used via Python operator overloading.
print(x)
print(x + y)
print(x * x)
print(x - y) 
print(x / y)

# Constants can be created via 
# FloatConst(name, mantissa_width, exponent_width)
# and then used for proofs.
x = SMTFloat.FloatConst("x", 23, 8)
y = SMTFloat.FloatConst("y", 23, 8)

# E.g. in floats always x > 0 & y > 0 => x * y >= 0
zero = SMTFloat.FloatValZero(FloatSort(23, 8))
solver = Solver()
condition = Implies(And(x > zero, y > zero), x * y >= zero)
solver.add(Not(condition))
print(solver.check())
\end{lstlisting}

\subsection{Testing}
The tests reside in \verb|tests/test.py|. Due to the costly validation against Z3 in many of these tests, they are best ran individually from the root directory. \\
\verb|python -m unittest tests.test.Operations.test_mul| runs a number of individual regression tests and then validates the floatsmt multiplication for half-floats against Z3.

\section{Project structure}
% TODO: superficially explain the different components of the library (sorts, operations, packing etc.) and the file/module organization. shortly explain api.py and its relation to the other stuff so we don't need to explain api.py later.
We will begin by outlining the conceptual structure of the library and then detail the concrete file structure. 

Each floating point value has an associated sort, which can roughly be understood as the type of the value. The sort contains the mantissa bit width and the exponent bit width for the floating point value. As such, floatsmt is entirely polymorphic over these widths. 
Given a sort, there are multiple different constructors for floating point values. These enable the creation of either floats with specific Python-/Z3 values, or float constants, which can be seen as existentially-quantified free variables. All of these constructors produce Z3 values.
Once a float value has been created, multiple operations and predicates can be used on it. The arithmetical operations are the most complex part of the library and work in multiple steps. First, both floats are pre-processed into a format that is easier to handle when calculating the result of the operations. Second, the corresponding operation is executed on the pre-processed float. Operations are fairly unrestricted in terms of how they can manipulate these preprocessed floats. Finally, the result of the operation is post-processed, where the result of the operation in the pre-processed format is converted back into a proper float.

Let us now explain the purpose of each file in the directory structure.
\begin{itemize}
	\item \verb|api.py|: Wraps user-facing functions and provides operator-overloading for some.
	\item \verb|constructors.py|: Provides functions that create floating point values of a specific sort, like a constructor that creates a float value from a string in decimal scientific notation.
	\item \verb|conversions.py|: Contains functions that convert floating point values to other Z3 sorts, like bit vectors or Z3 floats.
	\item \verb|converter.py|: Provides a function that parses a string in decimal notation and then converts the resulting decimal float to a binary float, rounding accordingly.
	\item \verb|operations.py|: Implements several arithmetical operations, like $+$, $*$, $|\cdot|$ or $\min$.
	\item \verb|packing.py|: Yields the pre- and post-processing utilities for arithmetical operations, as well as a function that translates between different float sorts.
	\item \verb|predicates.py|: Contains predicates that take floats and produce booleans, like $>=$ or $==$.
	\item \verb|sorts.py|: Implements the floating point sort and a sort for the rounding mode.
	\item \verb|utils.py|: Provides several utility functions, like a function to count leading zeros and several helper functions to ensure that bit vector operations always take arguments of the same width and never overflow when using arbitrary bit width floats.
\end{itemize}


\section{Implementation details}
% TODO: briefly note which functions were implemented and go into detail for interesting functions. explain how interesting functions were implemented, but also why certain design decisions were made.
This section will explain some design decisions and the implementation of all non-trivial functions in floatsmt.

\subsection{Sorts and constructors}
% sorts.py, constructors.py, converter.py
We use a Z3 \verb|DatatypeSort| as sort for our floats. Z3 datatypes provide similar capabilities as algebraic datatypes in other programming languages, and hence allow for multiple constructors, where each constructor stores a number of arguments. Our sort has the following signature:
\begin{lstlisting}
def FloatSort(mantissa_size : int, exponent_size : int) 
    -> DatatypeSortRef
\end{lstlisting}
We use a datatype with a single constructor \verb|mk| that takes three parameters: \verb|sign|, which is a bit vector with a singleton bit width, \verb|mantissa|, which designates a bit vector of the provided mantissa size and finally \verb|exponent|, which also represents a bit vector of a specified size. Z3 tuples are practically the same thing as datatypes with a single constructor, but the fields of a tuple are not named, and hence we chose datatypes for improved readability instead.
Since we also need to handle the different rounding modes within our Z3 theory, we define a Z3 \verb|EnumSort| that encapsulates the different cases.
For simplicity, we restrict the domains of all operations to floats with equal domains, and define a utility function \verb|ensure_eq_sort(a : DatatypeRef, b : DatatypeRef)| to enforce that invariant and throw an exception otherwise.

Most of the constructors are relatively straight-forward, essentially only invoking \verb|mk| before doing some small setup work. The three most fundamental constructors are the following:
\begin{lstlisting}
def FloatConst(name : str, mantissa_size : int, exponent_size : int) 
    -> DatatypeRef
def FloatVar(sign : BitVecRef, mantissa : BitVecRef, 
    exponent : BitVecRef, sort : DatatypeSortRef) -> DatatypeRef
def FloatVal(sign : int, mantissa : int, exponent : int, 
    sort : DatatypeSortRef) -> DatatypeRef
\end{lstlisting}
\verb|FloatVal| takes Python integers, while \verb|FloatVar| takes bit vectors as parameters.
All the other constructors are built on top of these three constructors. For instance, there is a constructor \verb|FloatVarBV| that takes a bit vector representation of a float and produces a floatsmt float, or a constructor \verb|FloatValPosInf| that creates a float representing $+\infty$.
One of the constructors is more involved: \verb|FloatValDec| takes a string representation of a float in decimal scientific notation and a rounding mode for rounding the decimal float to a binary float. In order to process the string parameter into a float, two steps are necessary: First, the float is parsed into a decimal float value of the form $m \cdot 10^e$. Second, the decimal float value is converted into a binary float of the form $m' \cdot 2^{e'}$. Since this conversion is not exact, it needs to be rounded in the process. The parsing is implemented with a regular manual lexer. The conversion is more complicated, since great care must be taken to accurately round the binary float without error. The conversion executes the following steps using Python's big integers:
\begin{enumerate}
	\item The float is normalized: $e$ is incremented and $m$ divided by $10$ until $10 \nmid m$. This way, trailing zeros in $m$ are moved to $e$.
	\item A tuple $(a, b)$ is created that represents the fraction $(a := m) / (10^{-e} =: b)$ if $e < 0$ and $(a := m \cdot 10^e) / (1 =: b)$ otherwise. It will later be used to determine the mantissa of the binary float and the remainder of the division for rounding.
	\item $\lfloor a / b \rfloor$ is scaled into the range $[2^M, 2^{M+1}) \subseteq \mathbb{N}$, where $M$ is the mantissa bit width of the resulting binary float and $[2^M, 2^{M+1})$ is the range that the mantissa needs to be in so that it is normalized. The fraction is scaled by either successively multiplying $a$ by $2$ if $\lfloor a / b \rfloor < 2^M$ or by multiplying $b$ by $2$ if $\lfloor a / b \rfloor \geq 2^{M+1}$. If $a'$ and $b'$ are the new numerator and denominator values, the amount of multiplication steps is tracked in a variable $c$ such that $a' / b' = 2^{-c} \cdot a / b$. This allows us to later adjust the exponent $e' := 2^{E-1}-1 + M + c$ accordingly, where $E$ denotes the exponent bit width, $2^{E-1}-1$ is the bias of the exponent and $M$ is added so that all mantissa bits are moved behind the decimal point. If we reach $e' \leq 0$ during scaling up, i.e. the float is subnormal, we re-adjust by scaling down only one step so that the exponent denotes a normal float. Subnormality will be handled later via the mantissa instead.
	\item $s := \lfloor a / b \rfloor$ and $r := a \text{ mod } b$ are evaluated, where $s$ denotes the significant and $r$ denotes the remainder. Using $r$, we increment $s$ according to the provided rounding mode. Rounding may yield $s = 2^{M+1}$, so if that occurs we set $s := 2^M$ and $c := c + 1$ and thus re-normalize the rounded $s$ into $[2^M, 2^{M+1})$.
	\item The mantissa $m'$ is set to be the binary representation of $s$. If $m'$ has less than $M$ bits, it must be subnormal, and hence we set $e' := 0$. Otherwise, we set $e'$ to be the binary representation of $2^{E-1}-1 + M + c$. If $e' = 2^E - 1$ or it has more than $E$ bits, the resulting float is too large to be represented by a float with $(M, E)$ bits and both $m'$ and $e'$ are set to the representation of $\infty$ to designate the overflow.
	\item Finally, we simply take the sign bit of the original decimal float.
\end{enumerate}

\subsection{Utilities and predicates}
% utils.py, predicates.py
In order to implement the arithmetical operations, we need a few utility functions. The first is a function \verb|clz(v : BitVecRef) -> BitVecRef| that computes the amount of leading zeros for a bit vector \verb|v| of size \verb|s| and can be understood as $\texttt{clz(v)} = \lfloor \log_2(\texttt{s}) \rfloor$. \verb|clz| progressively shifts \verb|v| to the right until no bits set to $1$ remain and then sums up the amount of remaining zeros. 
Bit vector functions also always require both arguments to have the same bit width, and hence we provide a function \verb|match_sizes| that takes a list of bit vectors and pads each one to the maximum width of a bit vector in the list. If the bit vector is unsigned, zeros are added to the left of the bit vector for padding. If it is signed, we add sign bits to the left instead.
Finally, when performing operations with arbitrary bit widths, it occurs frequently that an addition of the form $a + b$ can overflow. To remedy this, we implement a function \verb|guarantee_space(value, added, signed, offset)| that pads \verb|value| by enough bits so that $\texttt{value} + \texttt{added}$ cannot overflow. If $n$ is the bit width of \verb|value|, in the worst case we need to guarantee that $2^n + \texttt{added}$ does not overflow, i.e. we need to find a $k$ such that
\begin{align*}
2^{n + k} - 2^n &> \texttt{added} \iff \\
2^n \cdot (2^k - 1) &> \texttt{added} \iff \\
k &= \left\lfloor \log_2\left(\frac{\texttt{added}}{2^n} + 1\right) \right\rfloor + 1.
\end{align*}
Sometimes it is useful to offset the amount of added bits, using $k + \texttt{offset}$ for padding instead.

The different predicates are implemented as straight-forward logical expressions according to the IEEE 754 standard. We support bitwise equality with \verb|eq_bitwise|, predicates like \verb|is_nan| to query the state of a float and the typical relations like floating point equality via \verb|eq|, \verb|lt|, \verb|lte| and so on.

\subsection{Pre- and postprocessing}
% packing.py
Floats are pre-processed before each operation and the result of the operation is post-processed to produce the final float. The three-step process roughly works like the following:
\begin{enumerate}
	\item Every input float $(s, m, e)$ is unpacked. The implicit leading $1$ of the mantissa is prepended to $m$ if the float is normal and an implicit leading $0$ is prepended if it is subnormal. This way, both normals and subnormals can be handled in a uniform way in operations. Similarly, the $0$-exponent of floats is only used for identifying subnormals, while the real exponent is $1$. Hence, we set the exponent to $1$ during unpacking to allow for a uniform treatment of normals and subnormals. The bias $2^{E-1} - 1$ is subtracted from the exponent, which helps with performing operations on exponents. For example, when adding two exponent values, one would have to subtract the bias once afterwards if the biases were not subtracted beforehand. Next, the case of the float is identified. The case is an \verb|EnumSort|, denoting whether the float is $0$, $\infty$, \verb|NaN| or an unpacked normal float. The concrete case helps with preemptively distinguishing edge cases in each operation. Finally, the exponent is given two extra bits of precision so that operations on the exponent do not overflow as easily.
	\item First, the operation checks the case of the float and identifies the edge case of the output float. Then, the actual arithmetical operation is performed, producing an arbitrarily large exponent and a mantissa of the form $0.0\dots01x\dots xy\dots y$ with an arbitrary amount of leading zeros, $x \dots x$ representing the $M$ bits of the mantissa and $y \dots y$ representing the remainder of the operation that should be used for rounding.
	\item The output of the operation is packed back into a proper float. This involves shifting the mantissa accordingly, adjusting the exponent, rounding with the remainder, correctly converting subnormals, identifying overflows and underflows, as well as converting output edge cases of the operation back into their float representation. Since this process is quite involved, we will describe it separately.
\end{enumerate}

We will now describe the packing-process in detail. \verb|pack| has the following signature:
\begin{lstlisting}
def pack(f : DatatypeRef, sort : DatatypeSortRef, 
    rounding_mode : DatatypeRef, case : DatatypeRef) -> DatatypeRef
\end{lstlisting}
\verb|f| denotes the operation output that needs to be packed, \verb|sort| is the sort of the float that \verb|f| will get packed into, \verb|rounding_mode| denotes the mode with which to round using the remainder and \verb|case| is the edge case that the value falls into.
Let $M$ and $E$ designate the bit widths of \verb|sort|, $M_{\texttt{f}}$ and $E_{\texttt{f}}$ denote the bit widths of \verb|f|, $(s, m, e)$ represent the fields of \verb|f| and $(s', m', e')$ stand for the resulting float. $m$ is of the form $0.0\dots01x\dots xy\dots y$.
The process works as follows:
\begin{enumerate}
	\item The bias $2^{E-1}-1$ is added back to $e$ and both $m$ and $e$ are padded to the same bit width using \verb|match_sizes|. Let $\Delta m$ denote the amount of bits that were added to $m$ by \verb|match_sizes|.
	\item Even after adding the bias, the exponent may still be negative in its padded form because operations may produce an exponent that is smaller than $-(2^{E-1}-1)$. This can be fine if the resulting number is subnormal, in which case there are additional leading zeros at the front of the mantissa. We set $z_+ := \max(-e + 1, 0)$ for the amount of leading zeros to be added. Then, so as to prepend the leading zeros to the mantissa, $z_+$ bits need to get shifted out to the right. In order to not lose any precision for rounding the remainder, we calculate a sticky bit over the last $z_+ + 1$ bits via $s := (m\ \&\ (2^{z_+} - 1) \neq 0)$ and set the least significant bit of the mantissa after shifting to $s$. Finally, we adjust the exponent via $e := e + z_+$, resulting in $e \geq 1$. Note that shifting the mantissa may shift out all bits in $m$.
	\item Depending on $m$, we may also need to remove a number of leading zeros to normalize the mantissa. The amount of leading zeros after adding leading zeros but disregarding those added by \verb|match_sizes| is $z := \texttt{clz(m)} - \Delta m$. If $e > z$, we have enough space in our exponent to remove all of the leading zeros, whereas otherwise we can only remove $e - 1$ many zeros to maintain that $e \geq 1$. Hence, we set $z_- := z$ if $e > z$ and $z_- := e - 1$ otherwise for the amount of leading zeros to remove. Then, the width of the remainder bits $y \dots y$ is calculated as $R := M_{\verb|f|} - M - z_- - 1$ and the remainder $r$ extracted by shifting to the left until only $R$ bits remain. As a result of this construction, $r$ has some redundant trailing zeros, but since it will only be used for rounding, those do not need to be removed. Finally, the mantissa is shifted to the left by $z_-$ bits and the bits in the range $[M_f-1-M, M_f-2] \subseteq \mathbb{N}$ are extracted from $m$ to yield $m'$.
	\item Next, $m'$ is rounded using $s$, $r$ and the provided rounding mode. This may overflow the mantissa into a mantissa that only consists of zeros. If it does, the overflowed mantissa already has the correct value with $0$ after we set $e := e + 1$.
	\item Further, we test whether the resulting float is normal, i.e. whether $e > z$. If it is, we can use $e' := e - z$ for our exponent, otherwise we use $e' := 0$ to signal that the float is a subnormal. If the float is subnormal and $m' = 0$, we know that the operation underflowed. If $e \geq 2^E - 1$, we know that the operation overflowed. Unfortunately, this is not enough to determine whether we should set the resulting float to $\infty$: Using the \verb|Truncate| rounding mode, the \verb|Up| rounding mode on negative numbers or the \verb|Down| rounding mode on positive numbers, the float always needs to be rounded to the nearest representable float instead of overflowing. In any of these cases, we set the float to the nearest representable float. In the case of an underflow, it is set to $0$, and in the case of a regular overflow it is set to $\infty$. Outside of these edge cases, we extract the lower $E$ bits from $e'$.
	\item Finally, we check \verb|case| and yield the corresponding result. If the \verb|case| is an unpacked normal, we return the result from the previous steps.
\end{enumerate}

Using \verb|unpack| and \verb|pack|, we can also convert a float to a different floating point sort. The corresponding function has the following signature:
\begin{lstlisting}
def convert_float(a : DatatypeRef, new_sort : DatatypeSortRef, 
    rm : RoundingMode) -> DatatypeRef
\end{lstlisting}
Thanks to \verb|unpack| and \verb|pack|, the implementation of \verb|convert_float| is straight-forward: \verb|a| is unpacked and then packed into \verb|new_sort|. Unfortunately, if the sort of \verb|a| is smaller than \verb|new_sort|, adding the bias at the start of \verb|pack| may overflow the exponent. To resolve this, both the mantissa and the exponent of \verb|a| are padded to the sizes specified in \verb|new_sort|. The exponent is padded to the left, while the mantissa, which represents a couple of decimals, is padded to the right.
Then, we can unpack and pack to finish the conversion.
\subsection{Operations}
% TODO: operations.py

\section{Testing methodology}
% TODO: explain what was tested in test.py and how

\section{Experimental evaluation}
% TODO: evaluate our solver against z3 (and others?) and explain why it performs terribly and how one might improve it

\section{Conclusion}
% TODO: summarize the project and evaluate the outcome, putting emphasis on the correctness verified against z3

\bibliography{doc}

\appendix

\end{document}
