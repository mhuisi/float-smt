
\documentclass[a4paper,UKenglish,cleveref, autoref, thm-restate]{lipics-v2019}
%This is a template for producing LIPIcs articles. 
%See lipics-manual.pdf for further information.
%for A4 paper format use option "a4paper", for US-letter use option "letterpaper"
%for british hyphenation rules use option "UKenglish", for american hyphenation rules use option "USenglish"
%for section-numbered lemmas etc., use "numberwithinsect"
%for enabling cleveref support, use "cleveref"
%for enabling autoref support, use "autoref"
%for anonymousing the authors (e.g. for double-blind review), add "anonymous"
%for enabling thm-restate support, use "thm-restate"

%\graphicspath{{./graphics/}}%helpful if your graphic files are in another directory

\bibliographystyle{plainurl}% the mandatory bibstyle

\title{Documentation of floatsmt}

\author{Felix Griesau}{Karlsruhe Institute of Technology, Germany}{TODO}{}{}
\author{Marc Huisinga}{Karlsruhe Institute of Technology, Germany}{mhuisi@protonmail.com}{}{}

\authorrunning{F. Griesau and M. Huisinga}

\Copyright{Felix Griesau and Marc Huisinga}

\ccsdesc[500]{Theory of computation~Automated reasoning}

\keywords{Automated theorem proving, SMT solving, Z3, Floating point, IEEE 754, smtlib}

\category{}

\relatedversion{}

\supplement{\url{https://github.com/mhuisi/float-smt}}

\acknowledgements{}

\nolinenumbers

\hideLIPIcs 

\lstset{language=Python}

\begin{document}

\maketitle

\begin{abstract}
This report documents the floatsmt library, which implements a floating point theory using the Z3 SMT solver. 
\end{abstract}

% Use \cref for references and lstlisting for code.

\section{Introduction}
The IEEE 754 floating point standard \cite{ieee} is complex. Manually ensuring the correctness of programs that use floating point operations requires great care: 
\begin{itemize}
\item Edge cases like $+0$, $-0$, $+\infty$, $-\infty$ and \verb|NaN| need to be handled correctly, since all operations behave differently when applied to one of these edge cases.
\item Rounding errors need to be kept in check. Depending on the operation, the values and even the rounding mode involved, rounding errors may grow very large.
\item Operations on normals and subnormals can unintentionally produce one of the above edge cases in the situation of an underflow or an overflow.
\end{itemize}
Aside from these issues, floating point operations can always be understood in terms of their semantics, where a float is mapped to its real number value, the corresponding operation is performed over the reals and the result is rounded and mapped back to the floating point numbers.

Unfortunately, implementations of the floating point standard cannot easily and efficiently perform operations on the real numbers. Hence, implementations need to perform operations on floats using bit vectors and ensure that the result is still rounded correctly. This is even more error-prone than the semantical view of floats that users of these implementations work with, and very costly if erroneous\footnote{See \url{https://web.archive.org/web/20190618044444/http://www.trnicely.net/pentbug/pentbug.html} for an infamous example}. 

To combat these complexities, verification tools like SMT solvers have recently begun supporting floating point theories \cite{semantics}. These theories can be used to prove properties about implementations of the floating point standard and programs that use floats, ensuring correctness in regard to these properties.

But who watches the watchmen? How do we guarantee that the SMT solver implementation of the floating point theory is itself correct? After all, such an SMT solver implementation needs to surpass the same complexities as other floating point implementations.

To help reduce this uncertainty in the sense of ``trust, but verify'', we implemented floatsmt, a floating point theory for the Z3 SMT solver \cite{z3}. floatsmt was implemented from scratch, not based on any reference implementation and then verified against Z3's own floating point theory. In our verification, we found no bugs in Z3's theory, gaining a significant amount of trust in the floating point theory of Z3 in the process. This report details our decisions for the design and implementation of floatsmt.

\section{Usage instructions}
floatsmt is a Python library for the Z3 Python interface ``z3py''.

\subsection{Installation}
\begin{enumerate}
\item Install Python ($\geq$ 3.7.4) (\url{https://www.python.org/downloads/}).
\item Install the z3-solver package ($\geq$ 4.8.8.0) via \verb|pip install z3-solver| (\url{https://pypi.org/project/z3-solver/}).
\item Install git (\url{https://git-scm.com/}).
\item Clone the repository via \verb|git clone https://github.com/mhuisi/float-smt|.
\item Copy the \verb|floatsmt| directory to where the library will be used.
\item Add \verb|import floatsmt| to the top of the Python file where the library will be used and get started.
\end{enumerate} 

\subsection{Usage}
What follows is a code snippet to show how the library is used.
\begin{lstlisting}
from floatsmt.api import *

# Sets the rounding mode globally.
# Must be one of 
# {NearestTiesToEven,NearestTiesAwayFromZero,Up,Down,Truncate}.
set_default_rm(NearestTiesToEven)

# Create the floating point sort which we will use.
# floatsmt supports arbitrary bit widths.
# The first argument denotes the mantissa width,
# the second argument denotes the exponent width.
# (23, 8) is a standard 32 bit float.
sort = FloatSort(23, 8)

# Create a float constant from a specific bit representation.
x = SMTFloat.FloatVal(0, 0b1011, 0b1111, sort)
# Create a float constant from a decimal representation. 
# The mode for rounding from decimal to binary must be one of
# {NEAREST_TIE_TO_EVEN,NEAREST_TIE_AWAY_FROM_ZERO,UP,DOWN,TRUNCATE}.
y = SMTFloat.FloatValDec("0.123e-2", 
      converter.RoundingMode.NEAREST_TIE_TO_EVEN, sort)

# Many operations can be used via Python operator overloading.
print(x)
print(x + y)
print(x * x)
print(x - y) 
print(x / y)

# Constants can be created via 
# FloatConst(name, mantissa_width, exponent_width)
# and then used for proofs.
x = SMTFloat.FloatConst("x", 23, 8)
y = SMTFloat.FloatConst("y", 23, 8)

# E.g. in floats always x > 0 & y > 0 => x * y >= 0
zero = SMTFloat.FloatValZero(FloatSort(23, 8))
solver = Solver()
condition = Implies(And(x > zero, y > zero), x * y >= zero)
solver.add(Not(condition))
print(solver.check())
\end{lstlisting}

\subsection{Testing}
The tests reside in \verb|tests/test.py|. Due to the costly validation against Z3 in many of these tests, they are best ran individually from the root directory. \\
\verb|python -m unittest tests.test.Operations.test_mul| runs a number of individual regression tests and then validates the floatsmt multiplication for half-floats against Z3.

\section{Project structure}
% TODO: superficially explain the different components of the library (sorts, operations, packing etc.) and the file/module organization. shortly explain api.py and its relation to the other stuff so we don't need to explain api.py later.
We will begin by outlining the conceptual structure of the library and then detail the concrete file structure. 

Each floating point value has an associated sort, which can roughly be understood as the type of the value. The sort contains the mantissa bit width and the exponent bit width for the floating point value. As such, floatsmt is entirely polymorphic over these widths. 
Given a sort, there are multiple different constructors for floating point values. These enable the creation of either floats with specific Python-/Z3 values, or float constants, which can be seen as existentially-quantified free variables. All of these constructors produce Z3 values.
Once a float value has been created, multiple operations and predicates can be used on it. The arithmetical operations are the most complex part of the library and work in multiple steps. First, both floats are pre-processed into a format that is easier to handle when calculating the result of the operations. Second, the corresponding operation is executed on the pre-processed float. Operations are fairly unrestricted in terms of how they can manipulate these preprocessed floats. Finally, the result of the operation is post-processed, where the result of the operation in the pre-processed format is converted back into a proper float.

Let us now explain the purpose of each file in the directory structure.
\begin{itemize}
	\item \verb|api.py|: Wraps user-facing functions and provides operator-overloading for some.
	\item \verb|constructors.py|: Provides functions that create floating point values of a specific sort, like a constructor that creates a float value from a string in decimal scientific notation.
	\item \verb|conversions.py|: Contains functions that convert floating point values to other Z3 sorts, like bit vectors or Z3 floats.
	\item \verb|converter.py|: Provides a function that parses a string in decimal notation and then converts the resulting decimal float to a binary float, rounding accordingly.
	\item \verb|operations.py|: Implements several arithmetical operations, like $+$, $*$, $|\cdot|$ or $\min$.
	\item \verb|packing.py|: Yields the pre- and post-processing utilities for arithmetical operations, as well as a function that translates between different float sorts.
	\item \verb|predicates.py|: Contains predicates that take floats and produce booleans, like $>=$ or $==$.
	\item \verb|sorts.py|: Implements the floating point sort and a sort for the rounding mode.
	\item \verb|utils.py|: Provides several utility functions, like a function to count leading zeros and several helper functions to ensure that bit vector operations always take arguments of the same width and never overflow when using arbitrary bit width floats.
\end{itemize}


\section{Implementation details}
% TODO: briefly note which functions were implemented and go into detail for interesting functions. explain how interesting functions were implemented, but also why certain design decisions were made.
This section will explain some design decisions and the implementation of all non-trivial functions in floatsmt.

\subsection{Sorts and constructors}
% sorts.py, constructors.py, converter.py
We use a Z3 \verb|DatatypeSort| as sort for our floats. Z3 datatypes provide similar capabilities as algebraic datatypes in other programming languages, and hence allow for multiple constructors, where each constructor stores a number of arguments. Our sort has the following signature:
\begin{lstlisting}
def FloatSort(mantissa_size : int, exponent_size : int) 
    -> DatatypeSortRef
\end{lstlisting}
We use a datatype with a single constructor \verb|mk| that takes three parameters: \verb|sign|, which is a bit vector with a singleton bit width, \verb|mantissa|, which designates a bit vector of the provided mantissa size and finally \verb|exponent|, which also represents a bit vector of a specified size. Z3 tuples are practically the same thing as datatypes with a single constructor, but the fields of a tuple are not named, and hence we chose datatypes for improved readability instead.
Since we also need to handle the different rounding modes within our Z3 theory, we define a Z3 \verb|EnumSort| that encapsulates the different cases.
For simplicity, we restrict the domains of all operations to floats with equal domains, and define a utility function \verb|ensure_eq_sort(a : DatatypeRef, b : DatatypeRef)| to enforce that invariant and throw an exception otherwise.

Most of the constructors are relatively straight-forward, essentially only invoking \verb|mk| before doing some small setup work. The three most fundamental constructors are the following:
\begin{lstlisting}
def FloatConst(name : str, mantissa_size : int, exponent_size : int) 
    -> DatatypeRef
def FloatVar(sign : BitVecRef, mantissa : BitVecRef, 
    exponent : BitVecRef, sort : DatatypeSortRef) -> DatatypeRef
def FloatVal(sign : int, mantissa : int, exponent : int, 
    sort : DatatypeSortRef) -> DatatypeRef
\end{lstlisting}
\verb|FloatVal| takes Python integers, while \verb|FloatVar| takes bit vectors as parameters.
All the other constructors are built on top of these three constructors. For instance, there is a constructor \verb|FloatVarBV| that takes a bit vector representation of a float and produces a floatsmt float, or a constructor \verb|FloatValPosInf| that creates a float representing $+\infty$.
One of the constructors is more involved: \verb|FloatValDec| takes a string representation of a float in decimal scientific notation and a rounding mode for rounding the decimal float to a binary float. In order to process the string parameter into a float, two steps are necessary: First, the float is parsed into a decimal float value of the form $m \cdot 10^e$. Second, the decimal float value is converted into a binary float of the form $m' \cdot 2^{e'}$. Since this conversion is not exact, it needs to be rounded in the process. The parsing is implemented with a regular manual lexer. The conversion is more complicated, since great care must be taken to accurately round the binary float without error. The conversion executes the following steps using Python's big integers:
\begin{enumerate}
	\item The float is normalized: $e$ is incremented and $m$ divided by $10$ until $10 \nmid m$. This way, trailing zeros in $m$ are moved to $e$.
	\item A tuple $(a, b)$ is created that represents the fraction $(a := m) / (10^{-e} =: b)$ if $e < 0$ and $(a := m \cdot 10^e) / (1 =: b)$ otherwise. It will later be used to determine the mantissa of the binary float and the remainder of the division for rounding.
	\item $\lfloor a / b \rfloor$ is scaled into the range $[2^M, 2^{M+1}) \subseteq \mathbb{N}$, where $M$ is the mantissa bit width of the resulting binary float and $[2^M, 2^{M+1})$ is the range that the mantissa needs to be in so that it is normalized. The fraction is scaled by either successively multiplying $a$ by $2$ if $\lfloor a / b \rfloor < 2^M$ or by multiplying $b$ by $2$ if $\lfloor a / b \rfloor \geq 2^{M+1}$. If $a'$ and $b'$ are the new numerator and denominator values, the amount of multiplication steps is tracked in a variable $c$ such that $a' / b' = 2^{-c} \cdot a / b$. This allows us to later adjust the exponent $e' := 2^{E-1}-1 + M + c$ accordingly, where $E$ denotes the exponent bit width, $2^{E-1}-1$ is the bias of the exponent and $M$ is added so that all mantissa bits are moved behind the decimal point. If we reach $e' \leq 0$ during scaling up, i.e. the float is subnormal, we re-adjust by scaling down only one step so that the exponent denotes a normal float. Subnormality will be handled later via the mantissa instead.
	\item $s := \lfloor a / b \rfloor$ and $r := a \text{ mod } b$ are evaluated, where $s$ denotes the significant and $r$ denotes the remainder. Using $r$, we increment $s$ according to the provided rounding mode. Rounding may yield $s = 2^{M+1}$, so if that occurs we set $s := 2^M$ and $c := c + 1$ and thus re-normalize the rounded $s$ into $[2^M, 2^{M+1})$.
	\item The mantissa $m'$ is set to be the binary representation of $s$. If $m'$ has less than $M$ bits, it must be subnormal, and hence we set $e' := 0$. Otherwise, we set $e'$ to be the binary representation of $2^{E-1}-1 + M + c$. If $e' = 2^E - 1$ or it has more than $E$ bits, the resulting float is too large to be represented by a float with $(M, E)$ bits and both $m'$ and $e'$ are set to the representation of $\infty$ to designate the overflow.
	\item Finally, we simply take the sign bit of the original decimal float.
\end{enumerate}

\subsection{Utilities and predicates}
% utils.py, predicates.py
In order to implement the arithmetical operations, we need a few utility functions. The first is a function \verb|clz(v : BitVecRef) -> BitVecRef| that computes the amount of leading zeros for a bit vector \verb|v| of size \verb|s| and can be understood as $\texttt{clz(v)} = \lfloor \log_2(\texttt{s}) \rfloor$. \verb|clz| progressively shifts \verb|v| to the right until no bits set to $1$ remain and then sums up the amount of remaining zeros. 
Bit vector functions also always require both arguments to have the same bit width, and hence we provide a function \verb|match_sizes| that takes a list of bit vectors and pads each one to the maximum width of a bit vector in the list. If the bit vector is unsigned, zeros are added to the left of the bit vector for padding. If it is signed, we add sign bits to the left instead.
Finally, when performing operations with arbitrary bit widths, it occurs frequently that an addition of the form $a + b$ can overflow. To remedy this, we implement a function \verb|guarantee_space(value, added, signed, offset)| that pads \verb|value| by enough bits so that $\texttt{value} + \texttt{added}$ cannot overflow. If $n$ is the bit width of \verb|value|, in the worst case we need to guarantee that $2^n + \texttt{added}$ does not overflow, i.e. we need to find a $k$ such that
\begin{align*}
2^{n + k} - 2^n &> \texttt{added} \iff \\
2^n \cdot (2^k - 1) &> \texttt{added} \iff \\
k &= \left\lfloor \log_2\left(\frac{\texttt{added}}{2^n} + 1\right) \right\rfloor + 1.
\end{align*}
Sometimes it is useful to offset the amount of added bits, using $k + \texttt{offset}$ for padding instead.

The different predicates are implemented as straight-forward logical expressions according to the IEEE 754 standard. We support bitwise equality with \verb|eq_bitwise|, predicates like \verb|is_nan| to query the state of a float and the typical relations like floating point equality via \verb|eq|, \verb|lt|, \verb|lte| and so on.

\subsection{Pre- and postprocessing}
% packing.py
Floats are pre-processed before each operation and the result of the operation is post-processed to produce the final float. The three-step process roughly works like the following:
\begin{enumerate}
	\item Every input float $(s, m, e)$ is unpacked. The implicit leading $1$ of the mantissa is prepended to $m$ if the float is normal and an implicit leading $0$ is prepended if it is subnormal. This way, both normals and subnormals can be handled in a uniform way in operations. Similarly, the $0$-exponent of floats is only used for identifying subnormals, while the real exponent is $1$. Hence, we set the exponent to $1$ during unpacking to allow for a uniform treatment of normals and subnormals. The bias $2^{E-1} - 1$ is subtracted from the exponent, which helps with performing operations on exponents. For example, when adding two exponent values, one would have to subtract the bias once afterwards if the biases were not subtracted beforehand. Next, the case of the float is identified. The case is an \verb|EnumSort|, denoting whether the float is $0$, $\infty$, \verb|NaN| or an unpacked normal float. The concrete case helps with preemptively distinguishing edge cases in each operation. Finally, the exponent is given two extra bits of precision so that operations on the exponent do not overflow as easily.
	\item First, the operation checks the case of the float and identifies the edge case of the output float. Then, the actual arithmetical operation is performed, producing an arbitrarily large exponent and a mantissa of the form $0.0\dots01x\dots xy\dots y$ with an arbitrary amount of leading zeros, $x \dots x$ representing the $M$ bits of the mantissa and $y \dots y$ representing the remainder of the operation that should be used for rounding.
	\item The output of the operation is packed back into a proper float. This involves shifting the mantissa accordingly, adjusting the exponent, rounding with the remainder, correctly converting subnormals, identifying overflows and underflows, as well as converting output edge cases of the operation back into their float representation. Since this process is quite involved, we will describe it separately.
\end{enumerate}

We will now describe the packing-process in detail. \verb|pack| has the following signature:
\begin{lstlisting}
def pack(f : DatatypeRef, sort : DatatypeSortRef, 
    rounding_mode : DatatypeRef, case : DatatypeRef) -> DatatypeRef
\end{lstlisting}
\verb|f| denotes the operation output that needs to be packed, \verb|sort| is the sort of the float that \verb|f| will get packed into, \verb|rounding_mode| denotes the mode with which to round using the remainder and \verb|case| is the edge case that the value falls into.
Let $M$ and $E$ designate the bit widths of \verb|sort|, $M_{\texttt{f}}$ and $E_{\texttt{f}}$ denote the bit widths of \verb|f|, $(s, m, e)$ represent the fields of \verb|f| and $(s', m', e')$ stand for the resulting float. $m$ is of the form $0.0\dots01x\dots xy\dots y$.
The process works as follows:
\begin{enumerate}
	\item The bias $2^{E-1}-1$ is added back to $e$ and both $m$ and $e$ are padded to the same bit width using \verb|match_sizes|. Let $\Delta m$ denote the amount of bits that were added to $m$ by \verb|match_sizes|.
	\item Even after adding the bias, the exponent may still be negative in its padded form because operations may produce an exponent that is smaller than $-(2^{E-1}-1)$. This can be fine if the resulting number is subnormal, in which case there are additional leading zeros at the front of the mantissa. We set $z_+ := \max(-e + 1, 0)$ for the amount of leading zeros to be added. Then, so as to prepend the leading zeros to the mantissa, $z_+$ bits need to get shifted out to the right. In order to not lose any precision for rounding the remainder, we calculate a sticky bit over the last $z_+ + 1$ bits via $s := (m\ \&\ (2^{z_+} - 1) \neq 0)$ and set the least significant bit of the mantissa after shifting to $s$. Finally, we adjust the exponent via $e := e + z_+$, resulting in $e \geq 1$. Note that shifting the mantissa may shift out all bits in $m$.
	\item Depending on $m$, we may also need to remove a number of leading zeros to normalize the mantissa. The amount of leading zeros after adding leading zeros but disregarding those added by \verb|match_sizes| is $z := \texttt{clz(m)} - \Delta m$. If $e > z$, we have enough space in our exponent to remove all of the leading zeros, whereas otherwise we can only remove $e - 1$ many zeros to maintain that $e \geq 1$. Hence, we set $z_- := z$ if $e > z$ and $z_- := e - 1$ otherwise for the amount of leading zeros to remove. Then, the width of the remainder bits $y \dots y$ is calculated as $R := M_{\verb|f|} - M - z_- - 1$ and the remainder $r$ extracted by shifting to the left until only $R$ bits remain. As a result of this construction, $r$ has some redundant trailing zeros, but since it will only be used for rounding, those do not need to be removed. Finally, the mantissa is shifted to the left by $z_-$ bits and the bits in the range $[M_f-1-M, M_f-2] \subseteq \mathbb{N}$ are extracted from $m$ to yield $m'$.
	\item Next, $m'$ is rounded using $s$, $r$ and the provided rounding mode. This may overflow the mantissa into a mantissa that only consists of zeros. If it does, the overflowed mantissa already has the correct value with $0$ after we set $e := e + 1$.
	\item Further, we test whether the resulting float is normal, i.e. whether $e > z$. If it is, we can use $e' := e - z$ for our exponent, otherwise we use $e' := 0$ to signal that the float is a subnormal. If the float is subnormal and $m' = 0$, we know that the operation underflowed. If $e \geq 2^E - 1$, we know that the operation overflowed. Unfortunately, this is not enough to determine whether we should set the resulting float to $\infty$: Using the \verb|Truncate| rounding mode, the \verb|Up| rounding mode on negative numbers or the \verb|Down| rounding mode on positive numbers, the float always needs to be rounded to the nearest representable float instead of overflowing. In any of these cases, we set the float to the nearest representable float. In the case of an underflow, it is set to $0$, and in the case of a regular overflow it is set to $\infty$. Outside of these edge cases, we extract the lower $E$ bits from $e'$.
	\item Finally, we check \verb|case| and yield the corresponding result. If the \verb|case| is an unpacked normal, we return the result from the previous steps. For $s'$ we can directly use $s$.
\end{enumerate}

Using \verb|unpack| and \verb|pack|, we can also convert a float to a different floating point sort. The corresponding function has the following signature:
\begin{lstlisting}
def convert_float(a : DatatypeRef, new_sort : DatatypeSortRef, 
    rm : RoundingMode) -> DatatypeRef
\end{lstlisting}
Thanks to \verb|unpack| and \verb|pack|, the implementation of \verb|convert_float| is straight-forward: \verb|a| is unpacked and then packed into \verb|new_sort|. Unfortunately, if the sort of \verb|a| is smaller than \verb|new_sort|, adding the bias at the start of \verb|pack| may overflow the exponent. To resolve this, both the mantissa and the exponent of \verb|a| are padded to the sizes specified in \verb|new_sort|. The exponent is padded to the left, while the mantissa, which represents a couple of decimals, is padded to the right.
Then, we can unpack and pack to finish the conversion.
\subsection{Operations}
% operations.py
Here we will explain the more interesting operations in detail. The following descriptions are notably informal and intended to be read together with the source code. First, each operation enforces that the bit widths of all parameters are equal. Aside from the operations described individually below, floatsmt also provides \verb|abs|, \verb|neg|, \verb|min_float| and \verb|max_float| operations. The implementation of these is straight-forward as specified in the standard, with the small exception that the IEEE 754 standard does not specify the result of $\min(-0, +0)$, $\max(+0, -0)$, etc. This results in a small disparity between Z3 floats and floatsmt floats: Z3 does not reduce terms of the above form, while we assume $-0 < +0$ for the purpose of $\min$ and $\max$. Z3's \verb|roundToIntegral| is notably missing from the operations implemented by floatsmt for lack of time. A working implementation of \verb|roundToIntegral| could be extracted from the implementation of integer division in \cref{rem}.

\subsubsection{Addition}
% TODO: Felix

\subsubsection{Multiplication}
Thanks to \verb|unpack| and \verb|pack|, the implementation of \verb|mul| is fairly straight-forward. First, both arguments are unpacked and the output edge case is identified. 
Let $M$ and $E$ denote the bit widths of both mantissas and both exponents after unpacking.
Both mantissas are extended by $M$ bits to the left, since the multiplication of two mantissas of size $M$ can yield a mantissa of size $2M$ in the worst case. Then, the mantissas, each representing a number $x.x\dots x$, are multiplied. This multiplication yields a number $xx.x\dots x$. We add the exponents and increment the resulting exponent, since \verb|pack| takes a number $x.x \dots x$ and incrementing the exponent normalizes the mantissa into that form. Adding the exponents cannot overflow, since we added two extra bits of precision to the exponent in \verb|unpack|. Formally, this process works because $(m_1 \cdot 2^{e_1})(m_2 \cdot 2^{e_2}) = (m_1 \cdot m_2) \cdot 2^{e_1 + e_2}$. Finally, the output sign is the xor of the two input signs, and we can call \verb|pack| to handle the rest.

\subsubsection{Division}
Division (\verb|div|) is somewhat more involved. Once again, we first identify the output edge case and unpack the inputs. Let $M$ and $E$ denote the corresponding bit widths and both $(s_1, m_1, e_1)$ and $(s_2, m_2, e_2)$ represent the floats after unpacking. $(s', m', e')$ denotes the resulting float. In the following description, one should keep in mind that $(m_1 \cdot 2^{e_1}) / (m_2 \cdot 2^{e_2}) = (m_1 / m_2) \cdot 2^{e_1 - e_2}$.
Division then performs the following steps:
\begin{enumerate}
	\item Ideally, we would like to use the bit vector division and remainder operations to calculate the new mantissa and its remainder to round with. To achieve this, we pad $m_1$ to the right and $m_2$ to the left by $\Delta$ bits, gaining us $\Delta$ extra bits of precision when using bit vector division. If $m_1 = m_2$, we may need to shift $M$ bits to normalize the resulting $m_1 / m_2 = 1$ during \verb|pack|, so we need at least $M$ bits of additional precision in order to not lose all those extra decimal points that we need to represent in the mantissa. In the worst case however, $m_1$ is a subnormal and very small, while $m_2$ is very large, so we may even need $2M$ bits of additional precision to ensure that we have enough bits to fill the mantissa during normalization. We use a guard and a sticky bit for rounding correctly. The sticky bit will be determined later through the remainder, while the guard bit will be a by-product of the bit vector division. Hence, we set $\Delta := 2M + 1$.
	\item The divison is performed on the padded mantissas, yielding a quotient $q$ and a remainder $r$. The sticky bit is obtained via $s := r \neq 0$.
	\item $m'$ is finalized by appending $s$ to the right of $q$. We also set $e' := e_1 - e_2 + M - 1$, where $M - 1$ is added because the result of the division has $M$ leading digits. Before adding $M - 1$, we also need to ensure that the addition does not overflow, which is why we use \verb|guarantee_space| beforehand.
	\item The resulting sign bit is obtained via $s' := s_1 \oplus s_2$.
\end{enumerate}
Finally, we call \verb|pack| with $(s', m', e')$, finishing the operation.

\subsubsection{Fused Multiply-Add}
% TODO: Felix

\subsubsection{Remainder}
\label{rem}
The IEEE 754 remainder operation takes no rounding mode. For parameters $a$ and $b$ it roughly calculates $r := a - b \cdot (a / b)$, where $a / b$ is an integer division that rounds to the nearest integer using the nearest tie to even rounding mode and no additional rounding occurs in the intermediate operations. Only the result $r$ is rounded via the nearest tie to even rounding mode to the nearest representable float. Note that this may yield strange results, e.g. $6 \text{ rem } 4 = -2$.

In order to implement \verb|rem|, we first need to perform an integer division and then use \verb|fma| to perform the intermediate operations. Before executing the integer division, both inputs are unpacked and the result case is identified. Then, we preemptively need to ensure that the integer division does not overflow when packing: As we will see later, similar to \verb|div|, the exponent may get doubled and at most the mantissa bit width of the inputs is added to the exponent. Hence, we do not want the integer division to pack into our original sort, but instead an intermediate sort with an extra $1 + n$ bits, where $n$ is the amount of bits that \verb|guarantee_space| would suggest to add for the addition of $2^{e+1}$ and the mantissa bit width. This way, we do not run into any intermediate overflows during \verb|pack| calls. Similarly, to execute \verb|fma|, both inputs \verb|a| and \verb|b| need to get embedded into that same larger sort, which is done without loss of precision using \verb|convert_float|. Finally, the result of \verb|fma| is packed back into the smaller sort using \verb|convert_float| again, this time rounding in the process.

We will now describe the process for the integer division. Let $f_1$ and $f_2$ denote the two unpacked inputs to the integer division, $M$ and $E$ describe the mantissa- and exponent bit widths of the input, and $(s', m', e')$ represent the result.
\begin{enumerate}
	\item First, $f_1 / f_2$ is calculated similarly to \verb|div|, but without adjusting the exponent and packing at the end. Let this intermediate result be denoted by $(s'', m'', e'')$. Both $m''$ and $e''$ are matched to the same size. 
	\item If $M''$ is the size of $m''$, later down the line, we do not want $e'' + M''$ to overflow. Unfortunately, we also want $m''$ and $e''$ to have the same bit width, so first guaranteeing enough space to avoid the overflow and then matching the sizes of the result may again result in an overflow, since $M''$ is increased by matching the sizes. Luckily, we can resolve this mutual recursion by realizing that if $k$ extra bits are enough so that $e'' + M''$ never overflows, $k + 1$ extra bits are enough for $e'' + M'' + k + 1$ to not overflow. Specifically, if $E''$ denotes the bit width of $e''$, by the definition of \verb|guarantee_space|, we have $M'' < 2^{E''+k} - 2^{E''}$ and additionally $k + 1 \leq 2^{E''} \cdot (2^k - 1) = 2^{E'' + k} - 2^{E''}$ for $k, E'' \geq 1$. Hence, we have $M'' + k + 1 < 2 \cdot (2^{E'' + k} - 2^{E''}) = 2^{E'' + k + 1} - 2^{E'' + 1} \leq 2^{E'' + k + 1} - 2^{E''}$ and thus know that $e'' + M'' + k + 1$ will not overflow after extending the exponent by $k+1$ bits. We do this by calling \verb|guarantee_space| with an offset of $1$, matching the sizes of $m''$ and $e''$ and updating $M''$ to the new size of $m''$. 
	\item In order to round to the nearest integer, we need to convert $(m'', e'')$ into a canonical form that clearly distinguishes the integer from its decimal bits. First note that if $e'' = 0$, the integer is denoted by the leading digits of the mantissa as returned by the division, while the other bits are its decimal bits. If $e''$ is sufficiently large, all mantissa bits belong to the integer, and no decimal bits are left for rounding. Intuitively, we set $d := d'' + d_+$ for the amount of leading digits of the integer, where $d''$ denotes the amount of leading digits produced by the float division and $d_+$ represents the extra amount of leading digits that we need to convert the float into the canonical representation. Additionally, $R := M'' - d$ denotes the amount of remainder digits of the integer. $d''$ is defined by adding $M$ and the amount of leading digits added in the padding process of the first step. Let $R'' := M'' - d''$ represent the amount of remainder digits as produced by the float division. We choose $d_+ := \min(R'', e'')$: If $R'' \leq e''$, $e''$ is sufficiently large to turn all remainder bits into leading bits. Otherwise, we can only turn $e''$ many remainder digits into leading bits, and the rest are used for rounding the integer. Note that if $e''$ is sufficiently small and negative, both $d_+$ and even $d$ can be negative. Since $0 \leq d'' \leq M''$ and $-(2^{E''-1}-1) \leq d_+ \leq M'' - d''$, none of the above operations can overflow or underflow due to us guaranteeing the sufficient space for $M''$ in the previous step.
	\item The integer part $i$ of the mantissa is extracted by shifting the mantissa to the right by $R$ bits. If $d$ is negative, then $R > M''$ and thus $i = 0$. Similarly, the remainder $r$ is extracted by shifting the mantissa to the left by $d$ bits. If $d$ is negative, we instead shift to the right, losing some bits of precision. But this is not a problem: If $d$ is negative, the mantissa is of the form $0.0x \dots x$, and hence always rounded down later. 
	\item $i$ is rounded using the nearest tie to even rounding mode. This cannot overflow: If $i$ contains the entirety of the mantissa, then $r = 0$ and $i$ is not rounded up. Otherwise, the mantissa contains enough space so that rounding up does not overflow. The integer can however overflow $2^d - 1$ during rounding. If this occurs, we set $e'' := e'' + 1$ and $i := 2^{d-1}$ so that we do not lose any bits in the next step.
	\item $i$ is again shifted to the left by $R$ bits, producing $m'$, and $e' := e'' + d''$ is set so that the normalization by \verb|pack| produces the correct value. $d'' \leq M''$ and our \verb|guarantee_space| call guarantee that this does not overflow the exponent.
	\item Finally, $(s'', m', e')$ is packed with the round tie to nearest rounding mode.
\end{enumerate}
Note that despite the fact that we add $M'' > M$ to $e''$ in the worst case in step 6, the sort that we determine in \verb|rem| is still sufficient to prevent overflows in \verb|pack| during the integer division: $i$ has an extra $M'' - M$ leading zeros, which \verb|pack| removes during normalization, subtracting $M'' - M$ again and yielding at most the offset we determined earlier. 

\subsubsection{Square root}
% TODO: Felix

\section{Testing methodology}
Our testing methodology relies on verification against Z3 floats, using Z3 itself. During the development of floatsmt, we wrote the corresponding code and then implemented a validation call that uses Z3 to check whether the formula $\texttt{floatsmt\_output} = \texttt{z3\_output}$ is valid for all floats of a specific size. To compare the outputs, we convert our output to a Z3 float and then use bitwise equality to ensure that the results are equal. The validity check for a formula $F$ is a straight-forward satisfiability check of $\text{Not}(F)$. If $F$ is not valid, Z3 produces a counter-example, which we can then use to analyze our code. Additionally, for each counter-example, we added a regression test to our test code base. Trivial operations were only tested with a list of hand-crafted test cases.

We will henceforth denote a floating point sort with mantissa width $m$ and exponent width $e$ as $(m, e)$. In total, we tested and verified our code to the following degree:
\begin{itemize}
	\item The conversions from and to Z3 have been validated to be inverse to each other.
	\item All predicates have been tested for correctness in their edge cases.
	\item Float equality has been tested in its edge cases.
	\item The $>$ relation has been validated against Z3 for $(23, 8)$ floats.
	\item \verb|abs| and \verb|neg| have been tested in their edge cases.
	\item \verb|add|, \verb|mul| and \verb|div| have been validated against Z3 for all rounding modes and $(10, 5)$ floats.
	\item \verb|rem| has been validated against Z3 for roughly 20 hours on $(5, 3)$ floats and then stopped before completing the validation of the first rounding mode.
	\item \verb|sqrt| has been tested in numerous cases but eventually fails during validation on $(5, 3)$.
	\item \verb|fma| has been extensively tested but eventually fails during validation on $(5, 3)$ with a counter-example that cannot be reproduced in a regression test.
	\item \verb|min_float| and \verb|max_float| have been validated against Z3 on $(10, 5)$ floats, only yielding a disparity in the case where the IEEE 754 standard is not definite.
	\item \verb|pack| and \verb|unpack| have been validated to be inverse to each other on $(10, 5)$ and later $(5, 5)$ floats.
	\item \verb|convert_float| has been validated against Z3 for all possible options from \\ $\{(10, 5), (23, 8), (52, 11)\}$.
\end{itemize}

\section{Experimental evaluation}
% TODO: evaluate our solver against z3 (and others?) and explain why it performs terribly and how one might improve it
% TODO: Felix

\section{Conclusion}
While floatsmt did not end up being particularly efficient, it completes the task that it was created for: Ensuring that Z3's floating point theory is standard-conform and correct. With the exception of \verb|roundToIntegral|, all operations that are present in Z3's floating point theory were implemented in floatsmt and either tested extensively or validated against Z3. While a few of these validations could not be completed for performance reasons, we are confident that the performance issues and minor disparities can be repaired without too much effort. Finally, we believe that both this document and the codebase of floatsmt itself provide a useful and detailed reference to help with accurately and correctly implementing the floating point standard over bit vectors.

\bibliography{doc}

\appendix

\end{document}
